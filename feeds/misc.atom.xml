<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Do Not Disturb My Circles - misc</title><link href="https://donotdisturbmycircles.com/" rel="alternate"></link><link href="https://donotdisturbmycircles.com/feeds/misc.atom.xml" rel="self"></link><id>https://donotdisturbmycircles.com/</id><updated>2022-12-03T00:00:00+00:00</updated><entry><title>"Positional chess" is a weird concept</title><link href="https://donotdisturbmycircles.com/chess-probability.html" rel="alternate"></link><published>2022-12-03T00:00:00+00:00</published><updated>2022-12-03T00:00:00+00:00</updated><author><name>Movpasd</name></author><id>tag:donotdisturbmycircles.com,2022-12-03:/chess-probability.html</id><summary type="html"></summary><content type="html">&lt;p&gt;There are two methods of thinking when playing chess (or any abstract strategy
game) -- &lt;em&gt;calculational&lt;/em&gt; or &lt;em&gt;tactical&lt;/em&gt; thinking, and &lt;em&gt;heuristic&lt;/em&gt; or &lt;em&gt;strategic&lt;/em&gt;
thinking. In chess we might also call the latter &lt;em&gt;positional&lt;/em&gt; thinking.&lt;/p&gt;
&lt;p&gt;This is somewhat strange. After all, chess is a combinatorial game of perfect
information. If you lose, it is not because you rolled a dice imperfectly, nor
because you had to guess what an opponent would play and by misfortune guessed
poorly -- it is because you played a bad move. Therefore, it is strange
(at least to me) that heuristics should be involved at all.&lt;/p&gt;
&lt;p&gt;Here's it put another way. When you play chess, you constantly make decisions
based on &lt;em&gt;probability&lt;/em&gt;. When you decide to put a knight on a central square,
it's not because you know that 21 moves down the line you're going to be able
to threaten a fork. You put that knight there because you know that from a
central square, you're &lt;em&gt;more likely&lt;/em&gt; to be able to get it involved in a tactic
down the line.&lt;/p&gt;
&lt;p&gt;We are forced to make probabilistic decisions because the search space is far
too large due to the combinatorial nature of the game, despite there being
absolutely no probability whatsoever in the game itself. From the moment you
set up the pieces on the board, the game is one single extended tactical
combination, one long forced win or draw; and the only reason it is ever not
that is because at some point a player makes a mistake.&lt;/p&gt;
&lt;p&gt;You might think chess engines are different, but in fact they're not! Sure,
chess engines are able to calculate much, much deeper than humans, and that is
the main reason why they absolutely dominate human players; however, they
cannot calculate all the way down to checkmate from a complicated position
(and likely will never be able to barring massive improvements in computer
technology or the development of novel algorithms for reducing the search
space).&lt;/p&gt;
&lt;p&gt;Instead they look a couple of dozen moves deep (at most), and then &lt;em&gt;evaluate the
resulting position&lt;/em&gt;. And that evaluation function must be supplied by
a &lt;em&gt;human&lt;/em&gt;, based on centuries of &lt;em&gt;human&lt;/em&gt; chess position evaluation heuristics.
I mean, sure, a lot of it is common sense ("more pieces = more win"). But
because of this, chess engines often overlook fortresses and other clearly
drawn situations, insisting they might be able to squeeze out a win due to
superior material. In this sense, chess engines also have no choice but to
rely on probabilistic reasoning.&lt;/p&gt;
&lt;p&gt;I'm not sure how exactly to make rigorous, mathematical sense of this paradox.
How can a game without any probability require probabilistic reasoning? I'm
sure some mathematician specialising in game theory will have done research on
this. But I'll throw out an idea of how you might approach it.&lt;/p&gt;
&lt;p&gt;The idea is to borrow a page out of statistical mechanics and the notion of
microstate vs macrostate. You would divide the space of possible chess
positions (microstates) into parts (macrostates) according to various
properties of the position (such as material, piece activity, number of passed
pawns, and such). You would then argue that within each of these classes, a
certain fraction of the positions are won for White, a certain fraction are won
for Black, and a certain fraction are drawn. Evaluation of a position then
comes down to identifying the properties of the position and arguing that
because the position has this or that set of properties, it is more or
less &lt;em&gt;likely&lt;/em&gt; that White or Black is winning or that the game is drawn.&lt;/p&gt;
&lt;p&gt;The issue with this is that it just pushes the problem down one step. How do you
figure out those fractions without just calculating every position? What our
human brains do is they look back on all the games that we have played as well
as all the theory we have studied, and assign approximate probabilities to
these different macrostates. You could do the same thing: look at a database of
all games, look at every position that arises in those games, classify them
according to the properties you're studying, and derive win/draw percentages
from that.&lt;/p&gt;
&lt;p&gt;The issue with &lt;em&gt;that&lt;/em&gt; is that it effectively introduces a notion of "metagame".
It's conceivable that human play may get stuck in a metastable equilibrium, and
so that your win/draw proportions are not actually objective but depend on the
particularities of the metagame. Consider this as an illustration: if you fed
this model games exclusively from the 19th century, versus games exclusively
from the 21st, the former is likely to put much less stock on material and much
more on piece activity; this is not because the game has actually changed
since, but because of the &lt;em&gt;style of play&lt;/em&gt; that was prevalent at the time
(i.e.: the metagame).&lt;/p&gt;
&lt;p&gt;This "macrostate" idea is arguably quite close to the way actual humans evaluate
chess positions. We make &lt;em&gt;arguments&lt;/em&gt; as to why White or Black is winning in
this or that position, and then to support those arguments we point
to &lt;em&gt;reasons&lt;/em&gt;. Such reasons (such as "Black's king is exposed" or "White has a
great outpost on f6") are the macrostates.&lt;/p&gt;
&lt;p&gt;One last elaboration. Instead of considering only the ultimate evaluation of the
game based on a property of the current position, you could approach this
more "dynamically" by considering the probabilities of transitioning to other
positions with different properties. For example, a material imbalance of
20 to 21 is quite likely to eventually transition to one of 3 to 4, while it's
much less likely to end up at 19 to 12.&lt;/p&gt;</content><category term="misc"></category><category term="chess"></category><category term="games"></category><category term="maths"></category></entry><entry><title>CSS as a concatenative programming language</title><link href="https://donotdisturbmycircles.com/css-concatenative-semantics.html" rel="alternate"></link><published>2022-10-29T00:00:00+01:00</published><updated>2022-10-29T00:00:00+01:00</updated><author><name>Movpasd</name></author><id>tag:donotdisturbmycircles.com,2022-10-29:/css-concatenative-semantics.html</id><summary type="html"></summary><content type="html">&lt;p&gt;A &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Concatenative_programming_language"&gt;concatenative programming language&lt;/a&gt;&lt;/em&gt; is a programming language where the inputs and outputs of functions are not explicitly specified, and instead a program is defined by chaining them together. Concatenative-like syntax is common, like in fluent object-oriented interfaces or Unix pipes, as it's a quite natural way of thinking about computation: I have a value, and I want to pass it through a sequence of elements in order.&lt;/p&gt;
&lt;p&gt;I would argue this forms a natural model for representing the semantics of CSS selectors. But hang on -- aren't CSS selectors notoriously left-associative? I say notorious because this makes it &lt;a href="https://stackoverflow.com/questions/29757133/css-selector-for-adjacency"&gt;difficult to construct certain rules&lt;/a&gt;. If CSS selectors don't combine in a fully associative way, then they can't be understood as function composition.&lt;/p&gt;
&lt;p&gt;It seems to me this problem only arises because of the rather natural tendency to interpret CSS combinators like &lt;code&gt;&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;+&lt;/code&gt;, and such as being binary operators. And indeed, you can interpret the syntax in that way, and with the rules about left associativity you get a fully self-consistent semantics.&lt;/p&gt;
&lt;p&gt;But I think it's actually more natural to understand these operators as &lt;em&gt;combining with the selector on their right&lt;/em&gt; to form a single unit. So instead of parsing a selector like &lt;code&gt;div &amp;gt; p + .x&lt;/code&gt; as being &lt;code&gt;(div &amp;gt; p) + .x&lt;/code&gt; and then asking why it can't parse as &lt;code&gt;div &amp;gt; (p + .x)&lt;/code&gt;, you understand it as &lt;code&gt;(div) (&amp;gt; p) (+ .x)&lt;/code&gt;. If you do this, each sectioned unit can be understood as one independent function from the power set of DOM nodes to itself.&lt;/p&gt;
&lt;p&gt;You can understand this as being a form of unary operator, but I don't think this is the best way of thinking about it, as otherwise you'd need to add a restriction at the level of abstract syntax on why you can't apply it to a non-simple-sequence selector (&lt;code&gt;&amp;gt; (p + .x)&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;So, if &lt;code&gt;div&lt;/code&gt;, &lt;code&gt;&amp;gt; p&lt;/code&gt;, and &lt;code&gt;+ .x&lt;/code&gt; are functions, what do they operate on? They operate on &lt;em&gt;sets of DOM nodes&lt;/em&gt;. These DOM nodes are not unbound from their originating DOM document; if you like, they are by-reference, not by-value. This must be specified, because otherwise combination of CSS selectors could result in duplication of nodes, which is out-of-spec. The initial value is the set of all DOM nodes.&lt;/p&gt;
&lt;p&gt;This scheme offers something like a denotational semantics of what the CSS spec calls "complex selectors". It can be extended to lists of complex selectors, but in that case we leave the strict concatenative paradigm and enter the more general "function-level" paradigm, where the comma serves as something like a functional form.&lt;/p&gt;</content><category term="misc"></category><category term="code"></category><category term="short"></category></entry><entry><title>Inheritance is goto for polymorphism</title><link href="https://donotdisturbmycircles.com/inheritance-goto.html" rel="alternate"></link><published>2022-08-16T00:00:00+01:00</published><updated>2022-08-16T00:00:00+01:00</updated><author><name>Movpasd</name></author><id>tag:donotdisturbmycircles.com,2022-08-16:/inheritance-goto.html</id><summary type="html"></summary><content type="html">&lt;p&gt;Everyone likes to give inheritance a hard time these days. Compose, don't
inherit, so they say. Well, time for me to jump onto the bandwagon. Here is
reason #6701 why inheritance is a problem.&lt;/p&gt;
&lt;p&gt;Inheritance in traditional OOP introduces implicit single-dispatch polymorphism
at every call site. When you call &lt;code&gt;.myMethod()&lt;/code&gt; on &lt;code&gt;SomeClass someObject&lt;/code&gt;, you
may think you know what behaviour you're eliciting, but you actually don't.
Because if &lt;code&gt;SomeChildClass extends SomeClass&lt;/code&gt;, and later you or someone else
decides to &lt;code&gt;@override myMethod()&lt;/code&gt;, then the caller of &lt;code&gt;.myMethod()&lt;/code&gt; no longer
has any control over what happens.&lt;/p&gt;
&lt;p&gt;Contrast this with generics-based polymorphism. When I call a generic
&lt;code&gt;myProc&amp;lt;T&amp;gt;()&lt;/code&gt;, at the call site, I have full control over where control flow
goes. I know at compile time exactly what will be executed. This makes
&lt;code&gt;myProc&amp;lt;T&amp;gt;()&lt;/code&gt; nice and usable from the caller's perspective.&lt;/p&gt;
&lt;p&gt;Same thing for ad hoc polymorphism. When I call &lt;code&gt;myProc(float x)&lt;/code&gt;, I know that
it's not &lt;code&gt;myProc(String x)&lt;/code&gt; that will get run.&lt;/p&gt;
&lt;p&gt;(Not all inheritance has this problem, you specifically need dynamic dispatch,
i.e.: virtual methods. So this isn't as much of a problem in C++ for example,
since you know at the call site that unpredictable runtime polymorphism might
occur because you know can see the method is virtual.)&lt;/p&gt;
&lt;p&gt;Of course, the fact that you don't know where control flow is the whole &lt;em&gt;point&lt;/em&gt;
of OOP, right? Hide the implementation. Yes! Absolutely. Information hiding is
awesome. Modularity all the way. But, as has been pointed out by many before
me, the object just never seems to be the &lt;em&gt;right scale&lt;/em&gt; at which to create an
interface-implementation split. Modules are.&lt;/p&gt;
&lt;p&gt;This ends up incentivising code which is really hard to understand, where you
have to jump around between classes a lot. Because let's be honest: most
methods or procedures just don't have the kind of in-depth documentation of
exactly what it's supposed to do and which post-conditions need to be
maintained, information which is critical for a person &lt;code&gt;extend&lt;/code&gt;ing your class
in the future. So &lt;code&gt;TheirChildClass&lt;/code&gt; ends up violating that implicit
post-condition which callees of the method are quietly depending on.&lt;/p&gt;
&lt;p&gt;This reminds me of nothing less than the dreaded &lt;code&gt;goto&lt;/code&gt;. &lt;code&gt;goto&lt;/code&gt; is bad because
it creates an unstable relationship between the &lt;code&gt;goto&lt;/code&gt;er and the &lt;code&gt;goto&lt;/code&gt;ee. The
&lt;code&gt;goto&lt;/code&gt;er gets no guarantees about if, when, nor how execution will return to
it. This is &lt;em&gt;implicit control flow&lt;/em&gt;, because the &lt;code&gt;goto&lt;/code&gt; could send you anywhere
compared to the current statement, before or after, creating the potential for
complicated unexpected dependencies.&lt;/p&gt;
&lt;p&gt;With inheritance, you have &lt;em&gt;implicit polymorphism&lt;/em&gt;. Sure, you're still doing
structured programming, so at least you know the method will return. So there's
no uncertainty in the caller control flow. But there is unpredictability in
the &lt;em&gt;behaviour&lt;/em&gt; of the method call, which leaks upwards to make the caller
unpredictable as well.&lt;/p&gt;</content><category term="misc"></category><category term="code"></category></entry><entry><title>Software design is a lot like mathematics</title><link href="https://donotdisturbmycircles.com/code-like-maths.html" rel="alternate"></link><published>2022-08-13T00:00:00+01:00</published><updated>2022-08-13T00:00:00+01:00</updated><author><name>Movpasd</name></author><id>tag:donotdisturbmycircles.com,2022-08-13:/code-like-maths.html</id><summary type="html"></summary><content type="html">&lt;p&gt;I don't mean that programming is &lt;em&gt;mathematical&lt;/em&gt; (which it of course is), or that
mathematics is &lt;em&gt;mechanical&lt;/em&gt; (which it sometimes also is). I mean that the
actual process of software design and writing code is a lot like the process of
doing and creating mathematics.&lt;/p&gt;
&lt;p&gt;In both cases, you are trying to define the right structures so that they do the
right thing. In software design, you are defining the parts and interactions of
a complex system, to get it to produce the functionality desired. (If that
sounds gross and OOP to you, replace "parts and interactions" with "modules and
dependencies".) In mathematics, you're defining mathematical objects so that
they do something interesting and trying to prove it. In both cases, it's
trying to find the right definitions for the job.&lt;/p&gt;
&lt;p&gt;One of the key differences is that mathematicians &lt;em&gt;want&lt;/em&gt; their definitions to do
something exciting and unexpected. If a piece of software does something of the
sort, we usually call that a bug.&lt;/p&gt;
&lt;p&gt;But not always. There are certain software design approaches which value the
unexpected, usually by harnessing the emergence of functionality from the
juxtaposition of many small modules. Like the UNIX philosophy. Or functional
programming approaches, where power often comes from &lt;a href="https://stackoverflow.com/questions/6016271/"&gt;combinatoric explosion&lt;/a&gt; of definable functions. If you
had a reductionist bent, you could argue that mathematics is just the
exploration of the uncomputably large space of statements that may be derived
from axioms and logical inference rules. Good definitions are those which
make that space most accessible.&lt;/p&gt;
&lt;p&gt;I think there is a key takeaway from this for software design. A lot of
well-designed code is well-designed because it's designed with the pedantry and
aesthetic sensibilities of a mathematician. A well-designed piece of code
naturally covers its edge cases by the same token as a well-designed
definition.&lt;/p&gt;</content><category term="misc"></category><category term="code"></category><category term="maths"></category><category term="short"></category></entry><entry><title>You don't have furniture</title><link href="https://donotdisturbmycircles.com/dust.html" rel="alternate"></link><published>2022-08-06T00:00:00+01:00</published><updated>2022-08-06T00:00:00+01:00</updated><author><name>Movpasd</name></author><id>tag:donotdisturbmycircles.com,2022-08-06:/dust.html</id><summary type="html"></summary><content type="html">&lt;p&gt;Dust forms through the progressive degradation of the solids around us. It's an
uncomfortable reminder of a microscopic world that not only exists all around
us, but interacts with the more familiar macroscopic.&lt;/p&gt;
&lt;p&gt;In my scientific education, the idea that has changed my perspective on the
world around me most profoundly might have been my introductory materials
science course I took in my first year of university. It was quite cursory, of
course, but it made me realise that the world around me was this endless flux
of atoms and structure. A steel rod is not "merely" made of steel. It is steel
atoms arranged in an astoundingly complex arrangement of structure upon
structure upon microstructure.&lt;/p&gt;
&lt;p&gt;It was a moment that psychologists might call an "accommodation": The expansion
of one's internal model of reality to &lt;em&gt;accommodate&lt;/em&gt; new information that cannot
fit within one's existing models. I live for those moments. Moments when you
realise that you've been taking an idea for granted, an idea that in truth
merits closer inspection, and you perform that closer inspection and you become
a bigger person for it.&lt;/p&gt;
&lt;p&gt;I suppose the thing I'd been taking for granted was &lt;em&gt;solidness&lt;/em&gt;; &lt;em&gt;rigidity&lt;/em&gt;;
what it meant for an &lt;em&gt;object&lt;/em&gt; to be an object; or perhaps even &lt;em&gt;thingness&lt;/em&gt;, the
fact of being a single unified entity. A cup is a cup, and I move that whole
cup around when I move it. But in fact, it isn't. Rigidity is a concept that
exists only at a particular scale of size and stiffness. Try to pick up a metal
sheet a hundred metres across, or to grab water as if it were solid, and the
concept breaks down.&lt;/p&gt;
&lt;p&gt;I think this is quite a common learning obstacle in physics education. On
physics question-answer forums, I can't count the number of confusions I've
seen about reference frames, which goes something like this -- "if I'm in a
train, I'm in its reference frame; but if I leave it, at what point do I leave
the reference frame?" A question that I'm certain anyone who has studied
physics has had to ask themselves. Of course, the answer is that the reference
frame has nothing to do with the train's geometric boundaries. It's not a thing
you can be "in" or "out" of, the way you can be in or out of a room.&lt;/p&gt;
&lt;p&gt;In a similar vein, the "what if I have a rod a lightyear across and push one end
of it?" The expectation is that the far end of the rod should immediately begin
to move in response to the push, and it stems from a conceptualisation of the
rod as One Thing, with its associated concepts of indivisibility and rigidity.&lt;/p&gt;
&lt;p&gt;These are all questions I've asked myself at some point in my education, and
found partially satisfactory answers to, but it wasn't until that moment of
accommodation during some first-year Materials Science uni lecture that it all
clicked together. It was a &lt;em&gt;Matrix&lt;/em&gt; moment: "There is no spoon." Indeed, there
is no rod, there is no train, and there is no cup. There are only atoms and
void.&lt;/p&gt;
&lt;p&gt;Dust. It arises from the progressive degradation of the solid matter in our
environment. (Is it an inevitable consequence of the 2nd law of thermodynamics?
Indeed, to free up material from its rigid confinement in a solid material
structure is to allow its degrees of freedom to move more independently than
when it was in it. So it has more entropy, or at least, the potential for more
entropy. A pile of sand has more entropy than a chunk of quartzite.)&lt;/p&gt;
&lt;p&gt;Have you ever noticed your clothes getting a little threadbare, or the patterns
printed upon it degrading? Where did the threads of your once &lt;em&gt;threadplenty&lt;/em&gt;
favourite T-shirt go? They were pulverised. Perhaps not entire threads, perhaps
only the exterior layers of the threads still holding onto self-contained
Thingness, but pulverised, turned to dust on the counter, lint in the lint
trap, dust mite breakfast! (Or is it? Do dust mites eat fabric shreds?
Unsure.)&lt;/p&gt;
&lt;p&gt;The thingness of the things around us is an illusion. You don't have furniture.
There are atoms, and you think they are furniture. The exterior layers of the
wood of your shelf is incessantly sloughing off into the entropic soup we
unknowingly bathe in. With every scratch, every nick, every blemish, particles
of lignin are shredded and thrust into the air, where they will roll around and
degrade into chaotic strands of polymers, and from there through biological or
chemical processes broken into their constituent sugars and back out into the
ecological and geological cycles of the Earth.&lt;/p&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;p&gt;I don't like dust. I'm allergic.&lt;/p&gt;
&lt;p&gt;I should get back to dusting.&lt;/p&gt;</content><category term="misc"></category><category term="writing"></category><category term="physics"></category></entry><entry><title>Inertial forces are intrinsically Galilean</title><link href="https://donotdisturbmycircles.com/inertial-forces-invariant.html" rel="alternate"></link><published>2022-08-01T00:00:00+01:00</published><updated>2022-08-01T00:00:00+01:00</updated><author><name>Movpasd</name></author><id>tag:donotdisturbmycircles.com,2022-08-01:/inertial-forces-invariant.html</id><summary type="html"></summary><content type="html">&lt;p&gt;The defining property of inertial forces is that they don't affect Galilean invariants: distance and
duration.&lt;/p&gt;
&lt;p&gt;This &lt;em&gt;requires&lt;/em&gt; that they be proportional to mass, but that doesn't fully characterise them.&lt;/p&gt;
&lt;p&gt;The invariant definition can probably be proven to be equivalent to the more intuitive but arguably
less fundamental definition, that a time-dependent SO(3) transformation can remove them entirely.&lt;/p&gt;
&lt;p&gt;We can imagine some Galilean spacetime coordinate systems which introduce forces that might be
described as "hyperinertial". These would violate the integrity of rigid bodies -- there's probably
some connection to SO(3) here as well, what with rigid bodies being determined by an element of SO
(3) and a translation from the origin.&lt;/p&gt;
&lt;p&gt;I think Lorentzian relativity can't have inertial forces the way Galilean relativity does.
Invariance of the spacetime interval fixes the Poincar√© group (Lorentz group if origin fixed)
completely all on its own, whereas distance and duration &lt;em&gt;doesn't&lt;/em&gt; fix the Galilean group. You need
to impose linearity on top. This must have something to do with the fact that Galilean symmetry
decomposes directly between space and time, whereas Lorentzian only semidirectly.&lt;/p&gt;</content><category term="misc"></category><category term="short"></category><category term="physics"></category></entry><entry><title>Why multiple time dimensions don't work</title><link href="https://donotdisturbmycircles.com/multiple-time-dimensions.html" rel="alternate"></link><published>2022-07-30T00:00:00+01:00</published><updated>2022-07-30T00:00:00+01:00</updated><author><name>Movpasd</name></author><id>tag:donotdisturbmycircles.com,2022-07-30:/multiple-time-dimensions.html</id><summary type="html"></summary><content type="html">&lt;p&gt;In Lorentzian relativity, the (strict) past and future light cones are totally
disjoint, and no transformation within the part of SO&amp;NoBreak;(1, 3) connected
with the identity can mix these two sets.&lt;/p&gt;
&lt;p&gt;This property is so important that this part of SO&amp;NoBreak;(1, 3) is sometimes
called the "orthochronous" Lorentz group. It's what guarantees that the causal
ordering of events in spacetime is Lorentz invariant, and thus "objective".
(I'm tempted to bet that this also enforces some sort of existence and
uniqueness requirement on Cauchy initial value problems in Lorentzian
spacetime, further formalising the intuition behind causality, but don't quote
me on that.)&lt;/p&gt;
&lt;p&gt;Space does not have such a linear ordering. A rotation continuously connected to
the identity in SO&amp;NoBreak;(3) can rotate the +x axis into the -x axis.
Therefore, there can be no causal ordering of events in space. This also
reflects our intuition about causality: my left can be your right, but if a
particle's past from my perspective is its future from yours, we will disagree
about the order of causality, which would be a problem.&lt;/p&gt;
&lt;p&gt;The reason space doesn't have such a linear ordering is because there are more
than two space dimensions, which means that you can &lt;em&gt;rotate&lt;/em&gt; space. You can't
rotate time alone (in 1+3 dimensions) -- you can only rotate it along with
space (Lorentz boosts). The rotation group
SO&amp;NoBreak;(1) is trivial.&lt;/p&gt;
&lt;p&gt;If you had more than one time dimension, then you wouldn't really have a
spacetime, you'd have a two conjoined spaces of potentially different
dimensions, distinguished by their relative sign in the metric. Your "time"
dimension is nondeterministic. (Again, probably something here about Cauchy
initial value problems, but again, don't quote me on it.)&lt;/p&gt;
&lt;p&gt;(Of course, this ordering is ambiguous as to which direction is future and which
is past, and so doesn't imply an arrow of time. T symmetry still exists.)&lt;/p&gt;</content><category term="misc"></category><category term="short"></category><category term="physics"></category></entry></feed>